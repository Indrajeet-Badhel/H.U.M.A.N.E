# üß† H.U.M.A.N.E - Hyper-Understanding Multi-modal Assistant with Neural Emotions

H.U.M.A.N.E is an intelligent, multi-modal assistant that combines voice, image, and emotional tone to deliver human-like, emotionally adaptive responses. The system is designed for secure, context-aware conversations and interactive AI behaviors.

---

## üåü Features

- üéôÔ∏è Voice Input & Sentiment Analysis  
- üß† AI-Driven Prompt Conversion & Memory Handling  
- üì∏ Image Capture, Enhancement & Classification  
- üßæ Safety Guardrails for Explicit/Unsafe Inputs  
- üó£Ô∏è Emotion-Amplified Text-to-Speech Generation  
- üîÑ Multi-Turn Conversation Memory  
- üïµÔ∏è Personal Info Guard & Context Tracker  
- ‚ö° WebSocket-Driven Real-Time Communication  

---

## üóÇÔ∏è Project Structure

```bash
humane-ai/
‚îÇ
‚îú‚îÄ‚îÄ backend/ # Python backend logic (WebSocket, APIs)
‚îÇ ‚îú‚îÄ‚îÄ web_server.py
‚îÇ ‚îî‚îÄ‚îÄ websocket_handler.py
‚îÇ
‚îú‚îÄ‚îÄ web/ # Web UI
‚îÇ ‚îú‚îÄ‚îÄ assets/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ css/
‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ app.css
‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ landing.css
‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ styles.css
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ js/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ app.js
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ animations.js
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ landing.js
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ voice-controller.js
‚îÇ ‚îú‚îÄ‚îÄ app.html
‚îÇ ‚îî‚îÄ‚îÄ index.html
‚îÇ
‚îú‚îÄ‚îÄ voice_input/ # Voice capture & Whisper-based transcription
‚îú‚îÄ‚îÄ prompt_converter/ # Converts tone/image/text into structured prompts
‚îú‚îÄ‚îÄ safety_guard/ # Filters for explicit content and harmful data
‚îú‚îÄ‚îÄ tts_output/ # Emotion-based text-to-speech generation
‚îú‚îÄ‚îÄ templates/ # Optional HTML templates (if using Flask)
‚îú‚îÄ‚îÄ utils/ # Helper functions & memory management
‚îú‚îÄ‚îÄ main.py # Entry point (connects all agents)
‚îú‚îÄ‚îÄ agent_manager.py # Handles multi-agent pipeline logic
‚îú‚îÄ‚îÄ .env # API keys (Whisper, HuggingFace, Cohere, etc.)
‚îî‚îÄ‚îÄ pipeline_results.json # Stores latest conversation/memory info

```

---

## üöÄ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/humane-ai.git
cd humane-ai
```
2. Create & Activate Virtual Environment
```bash
python -m venv whisper_env
source whisper_env/Scripts/activate  # On Windows
```
3. Install Dependencies
```bash
pip install -r requirements.txt
```
You can generate requirements.txt with:
```bash
Copy code
pip freeze > requirements.txt
```
4. Set Environment Variables
Update .env with the following:
```bash
DEEPAI_API_KEY=your_key_here
HF_API_TOKEN=your_key_here
DANDELION_API_TOKEN=your_key_here
COHERE_API_KEY=your_key_here
```

## Functional Flow
css
```bash

[User Voice] --> [Whisper + Tone Analyzer] --> [Prompt + Image Handler] -->
[Memory Layer + Safety Guard] --> [LLM Agent(s)] --> [TTS Output (emotional)] --> [Voice Reply]
```
Voice Input is transcribed and tone-analyzed

Images (if any) are captured, enhanced, classified

Prompt converter ensures memory personalization and AI tuning

AI modules generate response, updated via conversation memory

Emotion Amplifier adjusts the voice response tone

## Web UI Interaction
HTML: /web/app.html

JS Controllers: app.js, voice-controller.js, animations.js

Styling: app.css, landing.css, styles.css

## Security & Ethics
‚ö†Ô∏è Explicit content is filtered at voice, text, and image levels

üß† AI does not store or share personal data without safety validation

‚è±Ô∏è Conversations are ephemeral unless user allows multi-turn memory
## Future Roadmap

 Docker + deployment script

 GPT-Free model switch (Ollama, LocalHF)

 Real-time webcam integration for emotional mirroring

 Progressive Web App (PWA) version

 GUI/Avatar for visual feedback

### Credits
Created by Indrajeet Badhel
üéì Vishwakarma Institute of Technology
üí° Focused on AI, Robotics, Blockchain

